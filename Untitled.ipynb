{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d670474",
   "metadata": {},
   "source": [
    "### E-commerce Insights: Data Prep, Cohorts, SQL Magic\n",
    "\n",
    "In the fast-changing world of online shopping, our data whiz, Alex, embarked on an exciting journey. With Python and SQL as trusted companions, he explored a vast e-commerce dataset in three key phases: Data Cleanup, Cohort Analysis, and SQL Insights.\n",
    "\n",
    "In the first phase, Data Cleanup with Python, Alex carefully combed through the dataset, fixing missing information and sorting out errors. He made sure the data was ready for the next steps.\n",
    "\n",
    "The second phase, Cohort Analysis with Python, was where things got interesting. Alex grouped customers based on their shopping habits and timelines. This revealed secrets about customer loyalty and how people shop online, like deciphering a hidden code.\n",
    "\n",
    "The final phase led Alex into the world of SQL Insights and Queries. Using SQL, he found answers to important questions. SQL helped Alex find these answers, giving us a full picture of e-commerce.\n",
    "\n",
    "With each line of code and every SQL query, Alex didn't just find answers; he paved the way for smarter decisions. This project wasn't just about numbers; it was about helping e-commerce thrive in a competitive world.\n",
    "\n",
    "Join us on this data-driven journey as we follow Alex's steps, revealing the secrets of online shopping success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4adebc8",
   "metadata": {},
   "source": [
    "### Module 1\n",
    "\n",
    "#### Task 1: Polishing the Dataset for Insights\n",
    "\n",
    "In the realm of e-commerce, data analyst Alex undertook the critical mission of transforming the \"transaction_dataset.csv\" into a strategic asset. He meticulously cleaned the data to ensure precision, eliminating extraneous columns such as \"product_class\" and \"product_size.\" Furthermore, he revamped column names to enhance clarity.\n",
    "\n",
    "The objective of this task was both simple and pivotal: to equip the organization with top-tier data for facilitating informed decision-making. It aimed to create a well-defined pathway towards data-driven insights that would steer the e-commerce platform toward resounding success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fad34d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transaction_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#--- Read in dataset ----\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ---WRITE YOUR CODE FOR TASK 1 ---\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_class\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_size\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transaction_dataset.csv'"
     ]
    }
   ],
   "source": [
    "#--- Import Pandas ---\n",
    "import pandas as pd \n",
    "\n",
    "#--- Read in dataset ----\n",
    "df = pd.read_csv(\"# give the path of your file \")\n",
    "\n",
    "# ---WRITE YOUR CODE FOR TASK 1 ---\n",
    "df.drop(['product_class','product_size'],axis=1,inplace=True)\n",
    "df.rename(columns={'tr_id': 'transaction_id', 'p_id': 'product_id', 'c_id': 'customer_id', 'tr_date': 'transaction_date'}, inplace=True)\n",
    "#--- Inspect data ---\n",
    "#df.to_csv('cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf25fa",
   "metadata": {},
   "source": [
    "In the ever-evolving realm of e-commerce, our mission is to unravel the intricate tapestry of customer behavior and engagement. We embark on a journey of data exploration and analysis, starting with the meticulous cleaning of our transaction dataset. By isolating and understanding 'Approved' transactions, we aim to uncover the patterns that lead to customer satisfaction and loyalty. Our voyage continues as we delve into cohort analysis, charting the trajectories of customer groups over time. Through these tasks, we seek to decipher the secrets of customer retention, identify growth opportunities, and ensure our e-commerce platform thrives in a dynamic digital landscape. This is a story of data-driven discovery, where each task brings us closer to delivering an exceptional online shopping experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32303831",
   "metadata": {},
   "source": [
    "### Module 2\n",
    "\n",
    "#### Task 1: Harvesting 'Approved' Transactions¶\n",
    "\n",
    "Cohort of Approved Transactions - Filtering for Excellence:\n",
    "\n",
    "At the heart of any data analysis lies the importance of clean and relevant data. In this first task, we're importing the necessary tools and filtering our dataset to focus exclusively on 'Approved' transactions. This is crucial because it helps us narrow down our analysis to the transactions that are most likely to provide insights into customer behavior and engagement, enabling us to make data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE FOR MODULE 2 TASK 1 ---\n",
    "\n",
    "approved_df = df[df['order_status']=='Approved']\n",
    "\n",
    "#--- Inspect data ---\n",
    "approved_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59a6c0",
   "metadata": {},
   "source": [
    "### Task 2: Transforming Date Data into Month Indices\n",
    "\n",
    "Once we've filtered our data, the next logical step is to extract and transform the specific information we need. By creating a new DataFrame and introducing features like the transaction date in 'YYYYMM' format and the transaction month index, we're preparing our data for deeper analysis. These features will help us understand when and how customers interact with our platform, laying the foundation for cohort analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b99a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have the 'approved_df' DataFrame\n",
    "\n",
    "# Step 2: Select 'customer_id' and 'transaction_date' columns from 'approved_df'\n",
    "filtered_df = approved_df[['customer_id', 'transaction_date']].copy()\n",
    "\n",
    "# Step 3: Convert the 'transaction_date' column to datetime\n",
    "filtered_df['transaction_date'] = pd.to_datetime(filtered_df['transaction_date'])\n",
    "\n",
    "# Step 4: Create a new 'YM' column with the year and month in \"%Y%m\" format\n",
    "filtered_df['YM'] = filtered_df['transaction_date'].dt.strftime('%Y%m')\n",
    "\n",
    "# Ensure the datatype of the 'YM' column is integer\n",
    "filtered_df['YM'] = filtered_df['YM'].astype(int)\n",
    "\n",
    "# Step 5: Determine the start month using the 'min()' function on the 'YM' column\n",
    "start_month = filtered_df['YM'].min()\n",
    "\n",
    "# Step 6: Calculate a new column 'transaction_month_index' by subtracting the start month value from each 'YM'\n",
    "filtered_df['transaction_month_index'] = filtered_df['YM'] - start_month\n",
    "\n",
    "# To view the results, type the name 'filtered_df' in your code\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ea84c",
   "metadata": {},
   "source": [
    "### Task 3: Unveiling Cohort Months: Identifying First Transaction Months for Customers\n",
    "\n",
    "Understanding the concept of cohort months is pivotal for cohort analysis. By determining when each customer joined a cohort, we're segmenting our customer base into meaningful groups. This task is essential because it establishes the basis for tracking customer behavior over time, allowing us to uncover trends and patterns within these cohorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b456a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- WRITE YOUR CODE FOR TASK 3 ---\n",
    "# Group data in 'filtered_df' by 'customer_id' and find the earliest 'transaction_month_index' for each customer using 'min()'\n",
    "cohort_month = filtered_df.groupby('customer_id')['transaction_month_index'].min().reset_index()\n",
    "\n",
    "# Rename the 'transaction_month_index' column to 'cohort_month'\n",
    "cohort_month = cohort_month.rename(columns={'transaction_month_index': 'cohort_month'})\n",
    "\n",
    "# Inspect the data by calling the variable 'cohort_month'\n",
    "cohort_month\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952e631",
   "metadata": {},
   "source": [
    "### Task 4: Cohort Connection: Merging Transaction Data with Customer Cohort Months\n",
    "\n",
    "To perform cohort analysis effectively, we need to merge our transaction data with the cohort information. Joining these DataFrames is crucial because it links each transaction to its respective cohort, forming the backbone of our cohort analysis. It enables us to trace how customer behavior changes over time within specific cohorts, providing invaluable insights for strategic decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...WRITE YOUR CODE FOR TASK 4 ...\n",
    "\n",
    "# Merge the 'filtered_df' DataFrame with the 'cohort_month' DataFrame based on the 'customer_id' column using the 'join()' function\n",
    "data = filtered_df.join(cohort_month.set_index('customer_id'), on='customer_id')\n",
    "\n",
    "# To access the merged dataset, refer to the 'data' variable\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205e264",
   "metadata": {},
   "source": [
    "### Task 5: Cohort Index Creation: Tracking Customer Transaction Month Relationships\n",
    "\n",
    "The cohort index represents the customer's journey within their cohort. Calculating this index is significant because it quantifies how long a customer has been a part of their cohort. This information is fundamental to cohort analysis as it allows us to compare the behavior of customers at various stages of their engagement, helping us identify trends and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE FOR TASK 5 ---\n",
    "# Create a new column 'cohort_index' by subtracting 'cohort_month' from 'transaction_month_index'\n",
    "data['cohort_index'] = data['transaction_month_index'] - data['cohort_month']\n",
    "\n",
    "#--- Inspect data ---\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97afd2e",
   "metadata": {},
   "source": [
    "### Task 6: Cohort Customer Count Analysis: Building 'final_df' for Insights\n",
    "This task helps us refine and summarize our cohort data. By creating the 'final_df,' we're simplifying our analysis, making it easier to interpret and visualize. This DataFrame contains the count of customers in each cohort at different time points, giving us a clear picture of customer retention and engagement patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- WRITE YOUR CODE FOR TASK 6 ---\n",
    "# Create 'df1' by selecting specific columns ('customer_id,' 'cohort_month,' and 'cohort_index') from the 'data'\n",
    "df1 = data[['customer_id', 'cohort_month', 'cohort_index']]\n",
    "\n",
    "# Remove duplicate rows in 'df1' based on specified columns and assign to 'df2'\n",
    "df2 = df1.drop_duplicates(subset=['customer_id', 'cohort_month', 'cohort_index'])\n",
    "\n",
    "\n",
    "# Generate 'final_df' by grouping unique rows in 'df2' based on 'cohort_month' and 'cohort_index,' and count unique customers\n",
    "final_df = df2.groupby(['cohort_month', 'cohort_index']).agg({'customer_id': 'count'}).reset_index()\n",
    "\n",
    "# Rename the 'customer_id' column to 'customer_count' for clarity\n",
    "final_df = final_df.rename(columns={'customer_id': 'customer_count'})\n",
    "\n",
    "# Reset the index of 'final_df' to ensure a structured format\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Inspect the data by calling the variable 'final_df'\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7310fcf",
   "metadata": {},
   "source": [
    "### Task 7: Cohort Data Transformation: Creating 'cohort_data' for Analytical Insights\n",
    "\n",
    "Cohort analysis involves studying how groups of customers evolve over time. The 'cohort_data' table is instrumental in this endeavor. It organizes customer counts by cohort and time, providing a structured view of customer behavior. This table is essential for creating visualizations and making data-driven decisions about marketing, product development, and customer retention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f712c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE FOR TASK 7 ---\n",
    "# Create a pivot table named 'cohort_data' from 'final_df'\n",
    "cohort_data = final_df.pivot_table(index='cohort_month', columns='cohort_index', values='customer_count')\n",
    "\n",
    "\n",
    "# View the pivot table 'cohort_data'\n",
    "cohort_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc8b3f",
   "metadata": {},
   "source": [
    "### Task 8: Cohort Analysis: Calculating Cohort Percentages for Evolution Insight  \n",
    "\n",
    "Percentages speak volumes when it comes to understanding customer engagement. Calculating cohort percentages allows us to see how each cohort's size changes over time relative to its initial size. This step is crucial because it helps us gauge the effectiveness of our strategies in retaining and engaging customers. Ultimately, it guides us in optimizing our efforts to improve customer loyalty and satisfaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aabe7e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cohort_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- WRITE YOUR CODE FOR TASK 8 ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate cohort percentage by dividing each value by the initial cohort size\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cohort_percentage \u001b[38;5;241m=\u001b[39m \u001b[43mcohort_data\u001b[49m\u001b[38;5;241m.\u001b[39mdivide(cohort_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Round the resulting values to three decimal places and multiply by 100 to get percentages\u001b[39;00m\n\u001b[0;32m      7\u001b[0m cohort_percentage \u001b[38;5;241m=\u001b[39m cohort_percentage\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cohort_data' is not defined"
     ]
    }
   ],
   "source": [
    "# --- WRITE YOUR CODE FOR TASK 8 ---\n",
    "\n",
    "# Calculate cohort percentage by dividing each value by the initial cohort size\n",
    "cohort_percentage = cohort_data.divide(cohort_data.iloc[:, 0], axis=0)\n",
    "\n",
    "# Round the resulting values to three decimal places and multiply by 100 to get percentages\n",
    "cohort_percentage = cohort_percentage.round(3) * 100\n",
    "\n",
    "# View the 'cohort_percentage' DataFrame\n",
    "cohort_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb373713",
   "metadata": {},
   "source": [
    "### Module 3\n",
    "\n",
    "#### Task 1: Data Download, Import, and Database Connection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the sql extention ----\n",
    "%load_ext sql\n",
    "\n",
    "# --- Load your mysql db using credentials from the \"DB\" area ---\n",
    "%sql mysql+pymysql://<b9257566>:<Cab#22se>@localhost/<cleaned_dataset>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce2794",
   "metadata": {},
   "source": [
    "### Task 2: What are the unique brands available in the dataset?\n",
    "We are enquiring about the unique brands available in the dataset to gain insights into the variety of products and manufacturers present in our sales records. This information can help us understand the market presence and popularity of different brands among our customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91002345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT DISTINCT brand \n",
    "FROM cleaned_dataset;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b16c1",
   "metadata": {},
   "source": [
    "### Task 3: How many unique customers made transactions in the dataset?\n",
    "\n",
    "We are investigating the number of unique customers who have made transactions in the dataset to assess the extent of our customer base. Understanding the total count of customers can aid in customer segmentation and targeted marketing efforts to enhance our business's customer relations and profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "# Count the number of unique customers with transactions\n",
    "SELECT COUNT(DISTINCT customer_id) AS unique_customers\n",
    "FROM cleaned_dataset;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70c54f",
   "metadata": {},
   "source": [
    "### Task 4: How many transactions were approved and how many were not approved?\n",
    "\n",
    "We are examining the number of transactions that were approved and those that were not approved to assess the overall success rate of transactions in our dataset. This information helps us evaluate our operational efficiency and customer satisfaction, as well as identify any potential issues that may require attention to improve the approval process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "#Calculate the total count of approved and unapproved transactions\n",
    "SELECT\n",
    "    SUM(CASE WHEN order_status = 'Approved' THEN 1 ELSE 0 END) AS approved_transactions,\n",
    "    SUM(CASE WHEN order_status <> 'Approved' THEN 1 ELSE 0 END) AS unapproved_transactions\n",
    "FROM cleaned_dataset;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53df9ce",
   "metadata": {},
   "source": [
    "### Task 5: List the top product lines with the highest average list price.\n",
    "\n",
    "We are inquiring about the top product lines with the highest average list price to identify the product categories that generate the highest revenue for our business. This knowledge can guide pricing strategies and marketing efforts, as well as help us understand customer preferences for premium products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "\n",
    "#List the top product lines with the highest average list prices\n",
    "SELECT\n",
    "    product_line,\n",
    "    AVG(list_price) AS average_list_price\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    product_line\n",
    "ORDER BY\n",
    "    average_list_price DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b9871",
   "metadata": {},
   "source": [
    "### Task 6: List the product_id, list_price, and standard_cost of the products where the list_price is greater than twice the standard_cost.\n",
    "\n",
    "We are querying for the product_id, list_price, and standard_cost of products where the list price is greater than twice the standard cost. This analysis helps us identify products with a significant profit margin, which is crucial for pricing decisions and profitability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "# List product_id, list_price, and standard_cost for products where list_price > 2 * standard_cost\n",
    "SELECT\n",
    "    product_id,\n",
    "    list_price,\n",
    "    standard_cost\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "WHERE\n",
    "    list_price > 2 * standard_cost;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0f11c",
   "metadata": {},
   "source": [
    "### Task 7: Calculate the average list_price for each product_line.\n",
    "We are calculating the average list price for each product line to understand the pricing trends within different product categories. This information can be valuable for setting competitive prices, evaluating product line performance, and making informed decisions related to product development and marketing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%sql\n",
    "\n",
    "\n",
    "SELECT avg(list_price) AS 'average_list_price',\n",
    "        product_line \n",
    "FROM cleaned_dataset \n",
    "Group by product_line "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0692f2",
   "metadata": {},
   "source": [
    "### Task 8: Which brand has the maximum difference between the list_price and standard_cost for their products?\n",
    "We are investigating which brand has the maximum difference between the list price and standard cost for their products. This analysis can help us identify brands with the potential for higher profit margins and evaluate pricing strategies across different manufacturers to optimize profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a66ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "    brand,\n",
    "    MAX(list_price - standard_cost) AS price_margin\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    brand\n",
    "ORDER BY\n",
    "    price_margin DESC\n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e474b9",
   "metadata": {},
   "source": [
    "### Task 9: List the customer_id and the count of their transactions, ordered by the number of transactions in descending order.¶\n",
    "We are listing the customer_id along with the count of their transactions, sorted in descending order by the number of transactions. This information can provide insights into customer behavior and loyalty, helping us identify our most valuable customers and tailor marketing strategies accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    customer_id,\n",
    "    COUNT(*) AS transaction_count\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    customer_id\n",
    "ORDER BY\n",
    "    transaction_count DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abd31a",
   "metadata": {},
   "source": [
    "### Task 10: Find the total sales amount for each brand, sorted in descending order of total sales.¶\n",
    "We are calculating the total sales amount for each brand, with the results sorted in descending order of total sales. This analysis allows us to identify the top-performing brands in terms of revenue generation, which is valuable information for marketing and inventory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b975cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    brand,\n",
    "    SUM(list_price) AS total_sales\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    brand\n",
    "ORDER BY\n",
    "    total_sales DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19702473",
   "metadata": {},
   "source": [
    "### Task 11: What are the top 5 products with the highest profit margin.\n",
    "We are identifying the top 5 products with the highest profit margin, where the profit margin is calculated as the difference between list_price and standard_cost. This analysis helps us pinpoint the most profitable products in our inventory, guiding pricing and marketing strategies to maximize profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "SELECT DISTINCT\n",
    "    product_id,\n",
    "    brand,\n",
    "    product_line,\n",
    "    (list_price - standard_cost) AS profit_margin\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "ORDER BY\n",
    "    profit_margin DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7d8a8",
   "metadata": {},
   "source": [
    "### Task 12: For each customer, find the total number of transactions, the total amount they spent, and their average profit per transaction\n",
    "We are calculating three metrics for each customer:\n",
    "\n",
    "Total number of transactions. Total amount spent (sum of list_price). Average profit per transaction (average profit per transaction = average list_price - average standard_cost). This analysis provides a comprehensive overview of each customer's transaction history, expenditure, and the average profitability of their purchases, helping us identify high-value customers and tailor marketing strategies accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "    customer_id,\n",
    "    COUNT(DISTINCT transaction_id) AS total_transactions,\n",
    "    SUM(list_price) AS total_amount_spent,\n",
    "    (AVG(list_price) - AVG(standard_cost)) AS average_profit_per_transaction\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    customer_id;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205a6ee",
   "metadata": {},
   "source": [
    "### Task 13: Find the top 5 product lines with the highest total revenue and their percentage contribution to the overall revenue.\n",
    "We are identifying the top 5 product lines with the highest total revenue (sum of list_price) and calculating their percentage contribution to the overall revenue. This analysis helps us understand which product lines are driving the most significant portion of our sales and revenue, enabling us to focus resources and strategies accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0325927",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "SELECT\n",
    "    product_line,\n",
    "    SUM(list_price) AS total_revenue,\n",
    "    (SUM(list_price) * 100 / (SELECT SUM(list_price) FROM cleaned_dataset)) AS revenue_contribution_percent\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    product_line\n",
    "ORDER BY\n",
    "    total_revenue DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e01780",
   "metadata": {},
   "source": [
    "### Task 14: Identify the customers who have made at least one transaction for each product line available\n",
    "We are identifying the customers who have made at least one transaction for each distinct product line available. This analysis helps us pinpoint the customers who have engaged with our entire product range, which can provide insights into their buying behavior and preferences across various product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "    customer_id\n",
    "FROM\n",
    "    cleaned_dataset\n",
    "GROUP BY\n",
    "    customer_id\n",
    "HAVING\n",
    "    COUNT(DISTINCT product_line) = (SELECT COUNT(DISTINCT product_line) FROM cleaned_dataset);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
